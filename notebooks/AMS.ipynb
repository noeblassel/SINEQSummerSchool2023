{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Multilevel Splitting (AMS)\n",
    "ANR SINEQ, Summer School, 09/27/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from time import perf_counter, sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Static Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the last particle algorithm, we consider the following toy example: we want to estimate the probability $p:=\\mathbb{P}(\\mathcal{N}(0,1)>q)$, with $q$ such that $p$ is very low. For this, we just need a transition kernel $P$ that is reversible wrt $\\mathcal{N}(0,1)$, for example a Gaussian kernel. Specifically, starting from $X$, this kernel proposes the transition to the position $\\rho X+\\sqrt{1-\\rho^2}W$, with $W\\sim\\mathcal{N}(0,1)$ independent of $X$ and $\\rho\\in(0,1)$ a tuning parameter. \n",
    "\n",
    "For a given level $L$, if $X\\sim\\mathcal{N}(0,1)|\\mathcal{N}(0,1)>L$, then\n",
    "\\begin{equation*}\n",
    "X':= \\begin{cases}\\rho X+\\sqrt{1-\\rho^2} W &\\textrm{ if $\\rho X+\\sqrt{1-\\rho^2} W>L$}\\\\ X&\\textrm{otherwise}\\end{cases} \n",
    "\\end{equation*}\n",
    "still satisfies $X'\\sim\\mathcal{N}(0,1)|\\mathcal{N}(0,1)>L$. Thus, if we apply a \"huge\" number of times $T$ this kernel, we obtain a new point that is still distributed according to $\\mathcal{N}(0,1)|\\mathcal{N}(0,1)>L$ and \"almost independent\" of the initial point $X$.\n",
    "\n",
    "Let us recall the idea of the algorithm: we begin with an $n$ i.i.d. sample with law $\\mathcal{N}(0,1)$ and, at each step, we compute the score $L$ of the less relevant particle (i.e., the minimum), we kill it and clone one of the $(n-1)$ other ones (randomly uniformly chosen), then we apply $T$ times to this clone the transition kernel, and so on until step $J_n:= \\max \\{j: L_j  \\leq q \\}$. The estimator of $p$ is then simply :\n",
    "$$\\hat p_n:=\\left(1-\\frac{1}{n}\\right)^{J_n}.$$\n",
    "In the so-called \"ideal\" case, we would apply an infinite number of times the transition kernel and would have the following CLT type result:\n",
    "$$\\sqrt{n}\\left(\\hat p_n-p\\right)\\xrightarrow[n\\to\\infty]{\\textrm{law}}\\mathcal{N}(0,-p^2\\log p),$$\n",
    "from which we easily deduce asymptotic confidence intervals for $p$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1 ####\n",
    "For $q=6$, $n=100$, $\\rho=0.9$, and $T=20$, implement this method to estimate $p:=\\mathbb{P}(\\mathcal{N}(0,1)>q)$, together with a $95\\%$ asymptotic confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "rho = 0.9\n",
    "T = 20\n",
    "q = 6\n",
    "alpha = 0.05\n",
    "p = sps.norm.sf(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 ####\n",
    "Use a histogram to compare the empirical distribution of the points $X_i$ obtained at the last step to the pdf of $\\mathcal{N}(0,1)|\\mathcal{N}(0,1)>q$. Take for example $n=1000$ in the previous code and the same values for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 ####\n",
    "We still consider the values $q=3$, $n=100$, $\\rho=0.9$, and $T=20$. Our goal is to illustrate the abovementioned CLT result. For this, apply $N=100$ times the previous method to get a sample of $N$ values of $\\hat p_n$. Compare the empirical standard deviation of this sample to the theoretical standard deviation of the CLT. By using the function `sps.gaussian_kde()`, compare the empirical distribution of the  (centered and normalized) sample to the pdf of the $\\mathcal{N}(0,1)$ law.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "n = 100\n",
    "rho = 0.9\n",
    "T = 20\n",
    "q = 3\n",
    "p = sps.norm.sf(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4 ####\n",
    "We now address the reverse problem, meaning that for a given (low) probability p, we want to estimate the $(1-p)$-associated quantile $q$, i.e. $\\mathbb{P}(\\mathcal{N}(0,1)>q)=p$. To do so, it suffices to apply the previous algorithm until step $j_n:= \\lceil \\frac{\\log p}{\\log (1-n^{-1})}\\rceil$ and set \n",
    "$$\\hat q_n:=L_{j_n}.$$\n",
    "In an ideal case, if $\\varphi$ stands for the pdf of the $\\mathcal{N}(0,1)$, we have\n",
    "$$\\sqrt{n}(\\hat{q}_n-q)\\xrightarrow[n\\to\\infty]{\\textrm{law}}\\mathcal{N}\\left(0,\\frac{-p^2\\log p}{\\varphi(q)^2}\\right).$$\n",
    "If we denote\n",
    "$$\\hat\\sigma_n:=\\frac{p\\sqrt{-\\log p}}{\\varphi(\\hat{q}_n)},$$\n",
    "this allows us to deduce asymptotic confidence intervals. \n",
    "\n",
    "For $p=10^{-6}$, $n=100$, $\\rho=0.9$, and $T=20$, implement the algorithm to estimate $q$ such that $\\mathbb{P}(\\mathcal{N}(0,1)>q)=p$, together with a $95\\%$ asymptotic confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "rho = 0.9\n",
    "T = 20\n",
    "p = 10**(-6)\n",
    "alpha = 0.05\n",
    "q = sps.norm.ppf(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.5 ####\n",
    "In most situations, we do not know the value $\\varphi(q)$ at the denominator of the TCL\n",
    "$$\\sqrt{n}(\\hat{q}_n-q)\\xrightarrow[n\\to\\infty]{\\textrm{law}}\\mathcal{N}\\left(0,\\frac{-p^2\\log p}{\\varphi(q)^2}\\right).$$\n",
    "However, if\n",
    "$$j_n^\\pm:= -n\\log p\\pm1.96\\sqrt{-n\\log p}$$\n",
    "then a $95\\%$ asymptotic confidence interval is \n",
    "$$\\left[L_{\\lfloor j_n^-\\rfloor},L_{\\lceil j_n^+\\rceil}\\right].$$\n",
    "To illustrate this result, apply $N=100$ times the previous algorithm to obtain $N$ asymptotic confidence intervals, and print the proportion of times that the true value $q$ belongs to the confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "n = 100\n",
    "rho = 0.9\n",
    "T = 20\n",
    "q = 3\n",
    "p = sps.norm.sf(q)\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Dynamic Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dimension 1, let us consider the SDE \n",
    "$$dX(t)=-V'(X(t)) dt + \\sqrt{2 \\varepsilon}\\ dB(t)$$\n",
    "where $V:\\mathbb R \\to \\mathbb R$ is a potential function, $\\varepsilon >0$ is a fixed parameter and $(B(t))_{t \\ge 0}$ stands for a standard Brownian motion. For two real numbers $a<b$, let $\\tau_a= \\inf\\{t \\ge 0, X(t)<a\\}$ and $\\tau_b= \\inf\\{t \\ge 0, X(t)>b\\}$. Given a fixed initial condition $X(0)=x_0 \\in (a,b)$, our goal is to estimate\n",
    "$$p=\\mathbb P(\\tau_b < \\tau_a).$$\n",
    "\n",
    "In this aim, we denote the timestep $\\delta_t>0$ and the following discretization scheme: for all $k \\ge 0$,\n",
    "$$X_{k+1} = X_k - V'(X_k) \\delta_t + \\sqrt{2 \\varepsilon \\delta_t}\\ G_k$$\n",
    "where $(G_k)_{k \\ge 0}$ is a sequence of i.i.d. standard Gaussian variables. Thus, we end up with the trajectory $(X_k)_{0 \\le k \\le T_a \\wedge T_b}$ where $T_a=\\min\\{k\\ge 0, \\, X_k < a\\}$ and $T_b=\\min\\{k\\ge 0, \\, X_k > b\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.0 #### \n",
    "Plot the graph of the double-well potential $V(x)=\\frac{x^4}{4}-\\frac{x^2}{2}$ for $x\\in[-1.5;1.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 #### \n",
    "Simulate and plot a trajectory $(X_k)_{0 \\le k \\le T_a \\wedge T_b}$ for the previous potential $V$ and the following values of the parameters: $a=-0.9$, $b=0.9$, $x_0=-0.89$, $\\varepsilon=0.02$, and $\\delta_t=0.001$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -0.9\n",
    "b = 0.9\n",
    "x0 = -0.89\n",
    "epsilon = 0.02\n",
    "dt = 0.001\n",
    "sigma = np.sqrt(2*epsilon*dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 #### \n",
    "Estimate $p=\\mathbb P(T_b<T_a)$ thanks to a naive Monte Carlo method (together with a $95\\%$ asymptotic confidence interval) for a sample size  $n=1000$, first for $\\varepsilon=1$, then for $\\varepsilon=0.02$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "alpha = 0.05\n",
    "sigma = np.sqrt(2*epsilon*dt)\n",
    "n = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 #### \n",
    "From now on, we apply the AMS algorithm. The initialization is as follows:\n",
    "* Simulate a list of $n$ trajectories $(X^i_k)_{0 \\le k \\le T_a \\wedge T_b}$, for $0 \\le i \\le n-1$,\n",
    "* Compute the first level $L_1=\\min_{0 \\le i \\le n-1}\\max_{0 \\le k \\le T_a \\wedge T_b} X^i_k$,\n",
    "* Determine the indices of the trajectories to be killed: $\\mathcal K=\\{0 \\le i \\le n-1,\\ \\max_{0 \\le k \\le T_a \\wedge T_b}X^i_k=L_1\\}$,\n",
    "* Initialize the probability estimate : $\\hat p_n=1-\\frac{|\\mathcal K|}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.02\n",
    "n = 10\n",
    "sigma = np.sqrt(2*epsilon*dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.4 #### \n",
    "Next, for each iteration, while $L \\le b$ and for each $i\\in \\mathcal K$:\n",
    "* Sample (randomly and uniformly) an index  $I'$ in $\\{0,\\ldots,n-1\\} \\setminus \\mathcal K$\n",
    "* Copy the trajectory $X^{I'}$ on $X^{i}$ until the first time $k$ such that $X^{I'}_k> L$ (beware to the strict inequality)\n",
    "* Complete this trajectory until time $T_a \\wedge T_b$\n",
    "* Update $L$, $\\mathcal K$ and the estimate $\\hat p_n \\leftarrow \\left(1-\\frac{|\\mathcal K|}{n}\\right)\\hat p_n$\n",
    "\n",
    "Implement this method for $\\varepsilon=1$ and for $\\varepsilon=0.02$, with $n=100$. For the asymptotic confidence interval, we will consider that only one trajectory is killed at each step so that, as in the first part, we have\n",
    "$$\\sqrt{n}\\left(\\hat p_n-p\\right)\\xrightarrow[n\\to\\infty]{\\textrm{law}}\\mathcal{N}(0,-p^2\\log p).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.005\n",
    "n = 200\n",
    "sigma = np.sqrt(2*epsilon*dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.5 #### \n",
    "In our setting, we can prove that the duration $D$ of a reactive trajectory satisfies\n",
    "$$D+\\log\\varepsilon\\xrightarrow[\\varepsilon\\to 0]{\\textrm{law}} G+c$$\n",
    "where $G$ has a standard Gumbel distribution and\n",
    "$$c=\\log(0.89 × 0.9)−\\frac{1}{2}\\log(1 − 0.89^2)−\\frac{1}{2}\\log(1 − 0.9^2).$$\n",
    "Roughly speaking, this means that, for $\\varepsilon$ low enough,\n",
    "$$D\\overset{\\textrm{law}}{\\approx} G-\\log\\varepsilon+c$$\n",
    "Use the code of the previous question to illustrate this result for $n=200$ and $\\varepsilon=0.005$. This might take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** : The theoretical result about the convergence toward a Gumbel distribution is established in   \n",
    "F. Cérou, A. Guyader, T. Lelièvre, and F. Malrieu, [On the Length of One-Dimensional Reactive Paths](https://perso.lpsm.paris/~aguyader/files/papers/cglm.pdf), *ALEA*, 2013.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
